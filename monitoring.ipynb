{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa6LX1/KagtioMbXkSPTqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olsem1/5609/blob/main/monitoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#settings.py\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "def last_day_of_month(day):\n",
        "    next_month = day.replace(day=28) + datetime.timedelta(days=4)\n",
        "    return next_month - datetime.timedelta(days=next_month.day)\n",
        "\n",
        "\n",
        "def report_and_publication_date():\n",
        "    today = datetime.datetime.now()\n",
        "    prev_date = datetime.datetime.now() - relativedelta(months=1)\n",
        "    report_date = last_day_of_month(prev_date).strftime(\"%d.%m.%Y\")\n",
        "    return report_date, today.strftime(\"%d.%m.%Y\")[2:], str(today.year)\n",
        "\n",
        "\n",
        "def all_datetypes_for_searching():\n",
        "    report_date_text, report_date_text_wo_day, publication_month_text = REPORT_DATE, REPORT_DATE[2:], PUBLICATION_MONTH\n",
        "\n",
        "    for old, new in REVERSE_REPLACEMENTS.items():\n",
        "        publication_month_text = publication_month_text.replace(new, old)\n",
        "    for old, new in REVERSE_REPLACEMENTS.items():\n",
        "        report_date_text = report_date_text.replace(new, old)\n",
        "    for old, new in REVERSE_REPLACEMENTS.items():\n",
        "        publication_month_text = publication_month_text.replace(new, old)\n",
        "    for old, new in REVERSE_REPLACEMENTS_IM.items():\n",
        "        report_date_text_wo_day = report_date_text_wo_day.replace(new, old)\n",
        "\n",
        "    return report_date_text, report_date_text_wo_day, publication_month_text\n",
        "\n",
        "\n",
        "LXML_PATH = 'lxml/'\n",
        "REPORT_DATE, PUBLICATION_MONTH, PUBLICATION_YEAR = report_and_publication_date()\n",
        "\n",
        "REVERSE_REPLACEMENTS = {\n",
        "    ' января ': '.01.',\n",
        "    ' февраля ': '.02.',\n",
        "    ' марта ': '.03.',\n",
        "    ' апреля ': '.04.',\n",
        "    ' мая ': '.05.',\n",
        "    ' июня ': '.06.',\n",
        "    ' июля ': '.07.',\n",
        "    ' августа ': '.08.',\n",
        "    ' сентября ': '.09.',\n",
        "    ' октября ': '.10.',\n",
        "    ' ноября ': '.11.',\n",
        "    ' декабря ': '.12.'}\n",
        "REPLACEMENTS = {\n",
        "    '\\xa0': '',\n",
        "    '\\t': '',\n",
        "    '&nbsp': '',\n",
        "    ' январь ': '.01.',\n",
        "    ' января ': '.01.',\n",
        "    ' февраль ': '.02.',\n",
        "    ' февраля ': '.02.',\n",
        "    ' марта ': '.03.',\n",
        "    ' март ': '.03.',\n",
        "    ' апреля ': '.04.',\n",
        "    ' апрель ': '.04.',\n",
        "    ' май ': '.05.',\n",
        "    ' мая ': '.05.',\n",
        "    ' июнь ': '.06.',\n",
        "    ' июня ': '.06.',\n",
        "    ' июль ': '.07.',\n",
        "    ' июля ': '.07.',\n",
        "    ' августа ': '.08.',\n",
        "    ' август ': '.08.',\n",
        "    ' сентябрь ': '.09.',\n",
        "    ' сентября ': '.09.',\n",
        "    ' октябрь ': '.10.',\n",
        "    ' октября ': '.10.',\n",
        "    ' ноябрь ': '.11.',\n",
        "    ' ноября ': '.11.',\n",
        "    ' декабрь ': '.12.',\n",
        "    ' декабря ': '.12.',\n",
        "    '.21': '.2021',\n",
        "    '.22': '.2022',\n",
        "    '-': '.',\n",
        "    '01.(': '01.{}'.format(REPORT_DATE[-4:]),\n",
        "    '02.(': '02.{}'.format(REPORT_DATE[-4:]),\n",
        "    '03.(': '03.{}'.format(REPORT_DATE[-4:]),\n",
        "    '04.(': '04.{}'.format(REPORT_DATE[-4:]),\n",
        "    '05.(': '05.{}'.format(REPORT_DATE[-4:]),\n",
        "    '06.(': '06.{}'.format(REPORT_DATE[-4:]),\n",
        "    '07.(': '07.{}'.format(REPORT_DATE[-4:]),\n",
        "    '08.(': '08.{}'.format(REPORT_DATE[-4:]),\n",
        "    '09.(': '09.{}'.format(REPORT_DATE[-4:]),\n",
        "    '10.(': '10.{}'.format(REPORT_DATE[-4:]),\n",
        "    '11.(': '11.{}'.format(REPORT_DATE[-4:]),\n",
        "    '12.(': '12.{}'.format(REPORT_DATE[-4:]),\n",
        "}\n",
        "MONTH_DICT = {\n",
        "    '.01.': '31.01.',\n",
        "    '.02.': '28.02.',\n",
        "    '.03.': '31.03.',\n",
        "    '.04.': '30.04.',\n",
        "    '.05.': '31.05.',\n",
        "    '.06.': '30.06.',\n",
        "    '.07.': '31.07.',\n",
        "    '.08.': '31.08.',\n",
        "    '.09.': '30.09.',\n",
        "    '.10.': '31.10.',\n",
        "    '.11.': '30.11.',\n",
        "    '.12.': '31.12.'\n",
        "}\n",
        "REVERSE_REPLACEMENTS_IM = {\n",
        "    ' январь ': '.01.',\n",
        "    ' февраль ': '.02.',\n",
        "    ' март ': '.03.',\n",
        "    ' апрель ': '.04.',\n",
        "    ' май ': '.05.',\n",
        "    ' июнь ': '.06.',\n",
        "    ' июль ': '.07.',\n",
        "    ' август ': '.08.',\n",
        "    ' сентябрь ': '.09.',\n",
        "    ' октябрь ': '.10.',\n",
        "    ' ноябрь ': '.11.',\n",
        "    ' декабрь ': '.12.',\n",
        "}\n",
        "\n",
        "REPORT_DATE_TEXT, REPORT_DATE_TEXT_WO_DAY, PUBLICATION_MONTH_TEXT = all_datetypes_for_searching()\n",
        "patterns = ['собственных средств', 'размер сс', 'расчет сс', 'размера сс', 'собственные средства',\n",
        "            'собственный средства']"
      ],
      "metadata": {
        "id": "9oyLv6HVhbtV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "B_gjJ0LZe3Zu"
      },
      "outputs": [],
      "source": [
        "#correct_links.py\n",
        "import re\n",
        "from re import search\n",
        "from urllib.parse import quote\n",
        "\n",
        "\n",
        "def correct_site_link(site_link_list):\n",
        "    for num, i in enumerate(site_link_list.copy()):\n",
        "        site_link_list[num] = re.split(',', site_link_list[num])[0].replace(\n",
        "            \"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"\").replace(\"www.\", \"\")\n",
        "    return site_link_list\n",
        "\n",
        "\n",
        "def full_download_link(download_link, url_index, site_links):\n",
        "    quote_dict = {\n",
        "        '%3F': '?',\n",
        "        '%3D': '=',\n",
        "        '%26': '&'\n",
        "    }\n",
        "    while download_link.startswith(\".\") or download_link.startswith(\"/\"):\n",
        "        download_link = download_link[1:]\n",
        "    if download_link.startswith(\"http\"):\n",
        "        return download_link\n",
        "    elif not download_link == '-':\n",
        "        if search(' ', download_link):\n",
        "            download_link = 'https://' + site_links[url_index] + \"/\" + quote(download_link)\n",
        "        else:\n",
        "            download_link = 'https://' + site_links[url_index] + \"/\" + download_link\n",
        "    for old, new in quote_dict.items():\n",
        "        download_link = download_link.replace(old, new)\n",
        "    return download_link\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_post_processing\n",
        "from re import search\n",
        "#from settings import REPORT_DATE, PUBLICATION_MONTH, REVERSE_REPLACEMENTS, PUBLICATION_YEAR\n",
        "import datetime\n",
        "\n",
        "post_correct_date_dict = {\n",
        "    \" янв \": \".01.\",\n",
        "    \" фев \": \".02.\",\n",
        "    \" мар \": \".03.\",\n",
        "    \" апр \": \".04.\",\n",
        "    \" май \": \".05.\",\n",
        "    \" июн \": \".06.\",\n",
        "    \" июл \": \".07.\",\n",
        "    \" авг \": \".08.\",\n",
        "    \" сен \": \".09.\",\n",
        "    \" окт \": \".10.\",\n",
        "    \" ноя \": \".11.\",\n",
        "    \" дек \": \".12.\",\n",
        "    \".\" + REPORT_DATE[-2:]: REPORT_DATE[-5:],\n",
        "    \".\" + PUBLICATION_MONTH[-2:]: PUBLICATION_MONTH[-5:]\n",
        "}\n",
        "\n",
        "\n",
        "def yes_no_result(href_list, yes_no_date):\n",
        "    for gen_num, i in enumerate(href_list):\n",
        "        if i == \"-\" or i is None:\n",
        "            yes_no_date.append(\"Нет\")\n",
        "        else:\n",
        "            yes_no_date.append(\"Да\")\n",
        "    return yes_no_date\n",
        "\n",
        "\n",
        "def correct_publication_dates(publication_date_list):\n",
        "    for index, date in enumerate(publication_date_list.copy()):\n",
        "        for old, new in REVERSE_REPLACEMENTS.items():\n",
        "            date = date.replace(\"\\xa0\", \" \").replace(old, new)\n",
        "        for old, new in post_correct_date_dict.items():\n",
        "            date = date.replace(\"\\xa0\", \" \").replace(old, new)\n",
        "        if search(PUBLICATION_MONTH, date):\n",
        "            publication_date_list[index] = datetime.datetime.strptime(date, \"%d.%m.%Y\").strftime(\"%d.%m.%Y\")\n",
        "        elif search(PUBLICATION_YEAR + PUBLICATION_MONTH[:3], date):\n",
        "            publication_date_list[index] = datetime.datetime.strptime(date, \"%Y.%m.%d\").strftime(\"%d.%m.%Y\")\n",
        "    return publication_date_list"
      ],
      "metadata": {
        "id": "5qgW3My-fC8P"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_post_processing.py"
      ],
      "metadata": {
        "id": "45Omec6Sf4fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_post_processing.py\n",
        "from re import search\n",
        "#from settings import REPORT_DATE, PUBLICATION_MONTH, REVERSE_REPLACEMENTS, PUBLICATION_YEAR\n",
        "import datetime\n",
        "\n",
        "post_correct_date_dict = {\n",
        "    \" янв \": \".01.\",\n",
        "    \" фев \": \".02.\",\n",
        "    \" мар \": \".03.\",\n",
        "    \" апр \": \".04.\",\n",
        "    \" май \": \".05.\",\n",
        "    \" июн \": \".06.\",\n",
        "    \" июл \": \".07.\",\n",
        "    \" авг \": \".08.\",\n",
        "    \" сен \": \".09.\",\n",
        "    \" окт \": \".10.\",\n",
        "    \" ноя \": \".11.\",\n",
        "    \" дек \": \".12.\",\n",
        "    \".\" + REPORT_DATE[-2:]: REPORT_DATE[-5:],\n",
        "    \".\" + PUBLICATION_MONTH[-2:]: PUBLICATION_MONTH[-5:]\n",
        "}\n",
        "\n",
        "\n",
        "def yes_no_result(href_list, yes_no_date):\n",
        "    for gen_num, i in enumerate(href_list):\n",
        "        if i == \"-\" or i is None:\n",
        "            yes_no_date.append(\"Нет\")\n",
        "        else:\n",
        "            yes_no_date.append(\"Да\")\n",
        "    return yes_no_date\n",
        "\n",
        "\n",
        "def correct_publication_dates(publication_date_list):\n",
        "    for index, date in enumerate(publication_date_list.copy()):\n",
        "        for old, new in REVERSE_REPLACEMENTS.items():\n",
        "            date = date.replace(\"\\xa0\", \" \").replace(old, new)\n",
        "        for old, new in post_correct_date_dict.items():\n",
        "            date = date.replace(\"\\xa0\", \" \").replace(old, new)\n",
        "        if search(PUBLICATION_MONTH, date):\n",
        "            publication_date_list[index] = datetime.datetime.strptime(date, \"%d.%m.%Y\").strftime(\"%d.%m.%Y\")\n",
        "        elif search(PUBLICATION_YEAR + PUBLICATION_MONTH[:3], date):\n",
        "            publication_date_list[index] = datetime.datetime.strptime(date, \"%Y.%m.%d\").strftime(\"%d.%m.%Y\")\n",
        "    return publication_date_list"
      ],
      "metadata": {
        "id": "AcCGKERQf8Gp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download_manager.py\n",
        "!pip install selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import InvalidArgumentException, TimeoutException\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def start_wd() -> webdriver.Firefox:\n",
        "    s = Service(r'geckodriver.exe')\n",
        "    options = Options()\n",
        "    options.set_preference(\"browser.download.folderList\", 2)\n",
        "    options.set_preference(\"browser.helperApps.alwaysAsk.force\", False)\n",
        "    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
        "    options.set_preference(\n",
        "        \"plugin.disable_full_page_plugin_for_types\", \"application/pdf\")\n",
        "    options.set_preference(\"pdfjs.disabled\", True)\n",
        "    options.set_preference(\n",
        "        \"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\")\n",
        "\n",
        "    browser = webdriver.Firefox(service=s, options=options)\n",
        "    browser.maximize_window()\n",
        "    browser.implicitly_wait(10)\n",
        "    browser.set_page_load_timeout(10)\n",
        "    return browser\n",
        "\n",
        "\n",
        "def save_page(path: str, num_page: int, lx_page) -> None:\n",
        "    with open(path + str(num_page) + 'page.lxml', 'w', encoding='utf-8') as parse_page:\n",
        "        parse_page.write(lx_page)\n",
        "\n",
        "\n",
        "def download_lxml(urls: list, lxml_path: str, browser) -> None:\n",
        "    error_link_list = []\n",
        "    error_link_index = []\n",
        "    for url_index, url in enumerate(urls):\n",
        "        print(url_index)\n",
        "        try:\n",
        "            browser.get(url)\n",
        "            WebDriverWait(browser, 10).until(\n",
        "                EC.presence_of_element_located((By.XPATH, '/html/body')))\n",
        "            page = browser.find_element(By.XPATH, '/html/body').get_attribute(\"innerHTML\")\n",
        "        except TimeoutException:\n",
        "            error_link_list.append(url)\n",
        "            error_link_index.append(url_index)\n",
        "            continue\n",
        "        except InvalidArgumentException:\n",
        "            pass\n",
        "        finally:\n",
        "            save_page(lxml_path, url_index, page)\n",
        "    browser.quit()\n",
        "    data = {'UK_num': error_link_index,\n",
        "            'URL': error_link_list}\n",
        "    pd.DataFrame(data).to_excel('inputs/error_link_list.xlsx', index=None)\n",
        "\n",
        "\n",
        "def download_error_lxml(lxml_path, browser) -> None:  # it's better to recheck urls by hands\n",
        "    df = pd.read_excel('inputs/error_link_list.xlsx')\n",
        "    error_link_dict = dict(zip(df['UK_num'].to_list(), df['URL'].to_list()))\n",
        "    for out_index, url in error_link_dict.items():\n",
        "        try:\n",
        "            browser.get(url)\n",
        "            WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, '/html/body')))\n",
        "            time.sleep(1.5)\n",
        "            page = browser.page_source\n",
        "            save_page(lxml_path, out_index, page)\n",
        "        except InvalidArgumentException:\n",
        "            pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmdFKkrPgKoN",
        "outputId": "9e28f6e5-9a5d-42c9-9d40-a31c3a773aaa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.7/dist-packages (from selenium) (2022.9.24)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.9.2)\n",
            "Requirement already satisfied: urllib3[socks]~=1.26 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.26.12)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.22.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.10)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get_download_link.py\n",
        "from re import search\n",
        "import re\n",
        "#from settings import REPORT_DATE, REPLACEMENTS, MONTH_DICT, PUBLICATION_MONTH\n",
        "\n",
        "\n",
        "def recursive_search_href(point, rec_list: list, floor=0):\n",
        "    try:\n",
        "        try:\n",
        "            href = point.find('a')['href']\n",
        "        except (TypeError, KeyError):\n",
        "            href = point['href']\n",
        "        rec_list.append(href)\n",
        "        return rec_list\n",
        "    except (TypeError, KeyError):\n",
        "        pass\n",
        "    if 5 <= floor:\n",
        "        return\n",
        "    parent = point.parent\n",
        "    recursive_search_href(parent, rec_list, floor + 1)\n",
        "    return rec_list\n",
        "\n",
        "\n",
        "def str_download_link(rss_list: list, soup_up) -> str:\n",
        "    for i in rss_list:\n",
        "        temp_str = i.replace(\"\\xa0\", \" \")\n",
        "        for old, new in REPLACEMENTS.items():\n",
        "            temp_str = temp_str.lower().replace(old, new)\n",
        "        if not search(\"31\", temp_str) and not search(\"30\", temp_str) and not search(\"29\", temp_str) and not \\\n",
        "                search(\"28\", temp_str) and not search(PUBLICATION_MONTH, temp_str):\n",
        "            for old, new in MONTH_DICT.items():\n",
        "                temp_str = temp_str.replace(old, new)\n",
        "        if search(REPORT_DATE, temp_str.replace(\" \", \"\")) or search(PUBLICATION_MONTH, temp_str):\n",
        "            try:\n",
        "                i = re.sub(\"(\\().*?\\)\", \"\", i)\n",
        "                href_str = soup_up.find('a', string=i)['href']\n",
        "                break\n",
        "            except TypeError:\n",
        "                try:\n",
        "                    u = soup_up.find(text=re.compile(i, re.I))\n",
        "                    rec_list = []\n",
        "                    href_str = recursive_search_href(u.parent, rec_list)[0]\n",
        "                    break\n",
        "                except AttributeError:\n",
        "                    for div in soup_up.find_all('div', {\"class\": \"name\"}):\n",
        "                        if search(PUBLICATION_MONTH, div.text):\n",
        "                            u = soup_up.find('div', text=div.text).parent.parent\n",
        "                            href_str = recursive_search_href(u.parent, rec_list)[0]\n",
        "                            break\n",
        "                    for span in soup_up.find_all('span', text=i):\n",
        "                        if search(REPORT_DATE, str(span)):\n",
        "                            href_str = span.parent['href']\n",
        "                            break\n",
        "    try:\n",
        "        href_str\n",
        "    except UnboundLocalError:\n",
        "        href_str = '-'\n",
        "    return href_str"
      ],
      "metadata": {
        "id": "RFR38C8AgWdN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_publication_dates_from_lxml.py\n",
        "import re\n",
        "from re import search\n",
        "import traceback\n",
        "#from settings import REPORT_DATE, REPORT_DATE_TEXT, REPORT_DATE_TEXT_WO_DAY, PUBLICATION_MONTH, PUBLICATION_MONTH_TEXT\n",
        "\n",
        "\n",
        "def recursive_search_pub_date(point, rec_list, floor=0):\n",
        "    pub_date = re.sub(\" +\", \" \", str(point.text).replace(\"\\n\", \" \").replace(\"-\", \".\").replace(\"/\", \".\").replace(\",\", \"\"))\n",
        "    if search(PUBLICATION_MONTH, pub_date) or\\\n",
        "        search(PUBLICATION_MONTH[:4] + PUBLICATION_MONTH[-2:], pub_date) or\\\n",
        "        search(PUBLICATION_MONTH[-4:] + PUBLICATION_MONTH[:4], pub_date) or\\\n",
        "        search(PUBLICATION_MONTH_TEXT, pub_date) or\\\n",
        "        search(PUBLICATION_MONTH[0:5] + PUBLICATION_MONTH[-1], pub_date) or\\\n",
        "        search(PUBLICATION_MONTH_TEXT.replace(\" \", \"\\xa0\"), pub_date) or\\\n",
        "        search(PUBLICATION_MONTH_TEXT[1:4] + ' ' + PUBLICATION_MONTH_TEXT[6:], pub_date) or\\\n",
        "        search(PUBLICATION_MONTH_TEXT[1:6] + PUBLICATION_MONTH_TEXT[-2:], pub_date):\n",
        "        try:\n",
        "            if search(PUBLICATION_MONTH, pub_date):\n",
        "                month_pos = pub_date.index(PUBLICATION_MONTH)\n",
        "                publication_str = pub_date[month_pos - 2:month_pos + 8]\n",
        "            elif search(PUBLICATION_MONTH_TEXT, pub_date) or \\\n",
        "                    search(PUBLICATION_MONTH_TEXT.replace(\" \", \"\\xa0\"), pub_date):\n",
        "                try:\n",
        "                    month_pos = pub_date.index(PUBLICATION_MONTH_TEXT)\n",
        "                except ValueError:\n",
        "                    month_pos = pub_date.index(PUBLICATION_MONTH_TEXT.replace(\" \", \"\\xa0\"))\n",
        "                publication_str = pub_date[month_pos - 2:month_pos + 10]\n",
        "            elif search(PUBLICATION_MONTH_TEXT[1:6] + PUBLICATION_MONTH_TEXT[-2:], pub_date):\n",
        "                month_pos = pub_date.index(PUBLICATION_MONTH_TEXT[1:6] + PUBLICATION_MONTH_TEXT[-2:])\n",
        "                publication_str = pub_date[month_pos - 3: month_pos + 7]\n",
        "            elif search(PUBLICATION_MONTH[-4:] + PUBLICATION_MONTH[0:4], pub_date):\n",
        "                month_pos = pub_date.index(PUBLICATION_MONTH[-4:] +\n",
        "                                           PUBLICATION_MONTH[:4])\n",
        "                publication_str = pub_date[month_pos:month_pos + 10]\n",
        "            elif search(PUBLICATION_MONTH[:4] + PUBLICATION_MONTH[-2:], pub_date):\n",
        "                month_pos = pub_date.index(PUBLICATION_MONTH[:4] + PUBLICATION_MONTH[-2:])\n",
        "                publication_str = pub_date[month_pos - 2:month_pos + 5] +\\\n",
        "                    PUBLICATION_MONTH[-3:]\n",
        "            elif search(PUBLICATION_MONTH_TEXT[1:4] + ' ' + PUBLICATION_MONTH_TEXT[6:], pub_date):\n",
        "                month_pos = pub_date.index(PUBLICATION_MONTH_TEXT[1:4] + ' ' + PUBLICATION_MONTH_TEXT[6:])\n",
        "                publication_str = pub_date[month_pos - 3:month_pos + 5] + \\\n",
        "                    PUBLICATION_MONTH[-3:]\n",
        "        except:\n",
        "            traceback.print_exc()\n",
        "            publication_str = ''\n",
        "        rec_list.append(publication_str)\n",
        "        return rec_list\n",
        "    if floor >= 23:  # 23 parents is too much, but for publication date with href it's ok\n",
        "        return\n",
        "    parent = point.parent\n",
        "    recursive_search_pub_date(parent, rec_list, floor + 1)\n",
        "    return rec_list\n",
        "\n",
        "\n",
        "def date_of_pub_parents(flat, soup):\n",
        "    for i in flat:\n",
        "        k = re.sub(\" +\", \" \", i.replace(\"\\xa0\", \" \"))\n",
        "        if (search(REPORT_DATE, k) \\\n",
        "            or search(REPORT_DATE[0:7] + REPORT_DATE[-1], k) \\\n",
        "            or search(REPORT_DATE_TEXT, k) or search(REPORT_DATE_TEXT_WO_DAY, k.lower())\n",
        "            or search(PUBLICATION_MONTH, k.lower())) or search((REPORT_DATE[0:-3] + REPORT_DATE[-1]).replace(\".\", \"\"), k)\\\n",
        "            and not search(\"уточнен\", i.lower()):\n",
        "            i = re.sub(\"(\\().*?\\)\", \"\", i)\n",
        "            rec_list = []\n",
        "            u = soup.find(text=re.compile(i[:97], re.I))\n",
        "            if \"\\xa0\" in str(u):\n",
        "                print('what to do')\n",
        "            try:\n",
        "                p = recursive_search_pub_date(u.parent, rec_list)[0]\n",
        "            except (IndexError, AttributeError):\n",
        "                traceback.print_exc()\n",
        "                p = '-'\n",
        "            break\n",
        "    try:\n",
        "        if p is None:\n",
        "            p = '-'\n",
        "    except UnboundLocalError:\n",
        "        p = '-'\n",
        "    return p"
      ],
      "metadata": {
        "id": "qrQB9M3UguQM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process_soup.py\n",
        "import re\n",
        "#from get_publication_dates_from_lxml import date_of_pub_parents\n",
        "#from get_download_link import str_download_link\n",
        "import traceback\n",
        "#from settings import *\n",
        "\n",
        "\n",
        "def form_raw_list_rss(soup, soup_up, xl_exception):\n",
        "    temp_list = []\n",
        "    if xl_exception == '-':\n",
        "        for element in patterns:\n",
        "            temp_list.append(soup_up.find_all(text=re.compile(element, re.I)))\n",
        "        flat = [x.replace('\\n', ' ').strip() for xs in temp_list for x in xs]\n",
        "        temp_str = '\\n'.join(flat)\n",
        "        if temp_str == '':\n",
        "            flat = ['-']\n",
        "        for i in flat.copy():\n",
        "            if len(i) > 400:\n",
        "                flat.remove(i)\n",
        "        href_str = str_download_link(flat, soup_up)\n",
        "        pub_str = date_of_pub_parents(flat, soup)\n",
        "    elif xl_exception == 'OnlyRSSByDate':\n",
        "        flat = soup_up.find_all(text=re.compile(REPORT_DATE))\n",
        "        href_str = str_download_link(flat, soup_up)\n",
        "        pub_str = date_of_pub_parents(flat, soup)\n",
        "    elif xl_exception == 'IncorYear':\n",
        "        temp_str = '\\n'.join(soup_up.find_all(text=re.compile(REPORT_DATE[0:-3] + REPORT_DATE[-1])))\n",
        "        if temp_str == '':\n",
        "            temp_str = '\\n'.join(soup_up.find_all(text=re.compile(REPORT_DATE[0:-3] \\\n",
        "                                                                  .replace(\".\", \"\") + REPORT_DATE[-1].replace(\".\",\n",
        "                                                                                                              \"\")))).replace(\n",
        "                \"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\")\n",
        "            flat = [re.sub(\"(\\().*?\\)\", \"\", temp_str)]\n",
        "            try:\n",
        "                href_str = soup.find(text=re.compile(temp_str, re.I)).parent['href']\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "            pub_str = date_of_pub_parents(flat, soup)\n",
        "        else:\n",
        "            flat = [re.sub(\"(\\().*?\\)\", \"\", temp_str)]\n",
        "            href_str = str_download_link(flat, soup_up)\n",
        "            pub_str = date_of_pub_parents(flat, soup)\n",
        "    elif xl_exception == 'LetterDate':\n",
        "        temp_str = '\\n'.join(soup_up.find_all(text=re.compile(REPORT_DATE_TEXT, re.I)))\n",
        "        flat = [temp_str]\n",
        "        href_str = str_download_link(flat, soup_up)\n",
        "        pub_str = date_of_pub_parents(flat, soup)\n",
        "    elif xl_exception == 'By_pub_date':\n",
        "        try:\n",
        "            flat = []\n",
        "            temp_str = soup.find(text=re.compile(PUBLICATION_MONTH)).parent.parent.parent.text.replace(\"\\n\", \"\") \\\n",
        "                           .replace(\"\\t\", \"\").strip().split(PUBLICATION_YEAR)[0] + PUBLICATION_YEAR\n",
        "            try:\n",
        "                flat.append(temp_str[temp_str.index(PUBLICATION_MONTH) - 2:temp_str.index(PUBLICATION_MONTH) + 8])\n",
        "            except ValueError:\n",
        "                flat.append(soup.find(text=re.compile(PUBLICATION_MONTH)))\n",
        "            pub_str = date_of_pub_parents(flat, soup)\n",
        "            href_str = str_download_link(flat, soup_up)\n",
        "        except AttributeError:\n",
        "            href_str = '-'\n",
        "            pub_str = '-'\n",
        "    elif xl_exception == 'rep_no_day':\n",
        "        flat = [soup.find(text=re.compile(REPORT_DATE_TEXT_WO_DAY, re.I))]\n",
        "        href_str = str_download_link(flat, soup_up)\n",
        "        pub_str = date_of_pub_parents(flat, soup)\n",
        "    elif xl_exception == 'By_rep_date_less_par':\n",
        "        try:\n",
        "            temp_str = '\\n'.join(soup.find_all(text=re.compile(REPORT_DATE)))\n",
        "            flat = [temp_str]\n",
        "            href_str = str_download_link(flat, soup_up)\n",
        "            pub_str = date_of_pub_parents(flat, soup)\n",
        "        except AttributeError:\n",
        "            flat.append('-')\n",
        "            pub_str = date_of_pub_parents(flat, soup)\n",
        "    elif xl_exception == 'Не удалось':\n",
        "        href_str = '-'\n",
        "        pub_str = '-'\n",
        "    elif xl_exception == 'rep_no_year':\n",
        "        try:\n",
        "            u = soup.find(text=re.compile(REPORT_DATE_TEXT, re.I)).parent.parent\n",
        "            href_str = u.find('a')['href']\n",
        "            str_find = u.find(text=re.compile(PUBLICATION_MONTH, re.I))\n",
        "            month_pos = str_find.index(PUBLICATION_MONTH)\n",
        "            pub_str = str_find[month_pos - 2:month_pos + 8]\n",
        "        except TypeError:\n",
        "            report_date_text_2 = REPORT_DATE_TEXT[:-5]\n",
        "            u = soup.find(text=re.compile(\"средств на \" + report_date_text_2, re.I)).parent.parent.parent.parent.parent\n",
        "            temp_str = u.find(text=re.compile(PUBLICATION_MONTH, re.I))\n",
        "            pub_str = temp_str\n",
        "            href_str = u.find('a')['href']\n",
        "    elif xl_exception == 'pub_date_inside':\n",
        "        for element in patterns:\n",
        "            temp_list.append(soup_up.find_all(text=re.compile(element, re.I)))\n",
        "        flat = [x.replace('\\n', ' ').strip() for xs in temp_list for x in xs]\n",
        "        flat.reverse()\n",
        "        for i in flat.copy():\n",
        "            if not PUBLICATION_MONTH in i:\n",
        "                flat.remove(i)\n",
        "        href_str = str_download_link(flat, soup_up)\n",
        "        pub_str = date_of_pub_parents(flat, soup)\n",
        "    return pub_str, href_str"
      ],
      "metadata": {
        "id": "3wZMPOQMhOd9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#settings.py \n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "def last_day_of_month(day):\n",
        "    next_month = day.replace(day=28) + datetime.timedelta(days=4)\n",
        "    return next_month - datetime.timedelta(days=next_month.day)\n",
        "\n",
        "\n",
        "def report_and_publication_date():\n",
        "    today = datetime.datetime.now()\n",
        "    prev_date = datetime.datetime.now() - relativedelta(months=1)\n",
        "    report_date = last_day_of_month(prev_date).strftime(\"%d.%m.%Y\")\n",
        "    return report_date, today.strftime(\"%d.%m.%Y\")[2:], str(today.year)\n",
        "\n",
        "\n",
        "def all_datetypes_for_searching():\n",
        "    report_date_text, report_date_text_wo_day, publication_month_text = REPORT_DATE, REPORT_DATE[2:], PUBLICATION_MONTH\n",
        "\n",
        "    for old, new in REVERSE_REPLACEMENTS.items():\n",
        "        publication_month_text = publication_month_text.replace(new, old)\n",
        "    for old, new in REVERSE_REPLACEMENTS.items():\n",
        "        report_date_text = report_date_text.replace(new, old)\n",
        "    for old, new in REVERSE_REPLACEMENTS.items():\n",
        "        publication_month_text = publication_month_text.replace(new, old)\n",
        "    for old, new in REVERSE_REPLACEMENTS_IM.items():\n",
        "        report_date_text_wo_day = report_date_text_wo_day.replace(new, old)\n",
        "\n",
        "    return report_date_text, report_date_text_wo_day, publication_month_text\n",
        "\n",
        "\n",
        "LXML_PATH = 'lxml/'\n",
        "REPORT_DATE, PUBLICATION_MONTH, PUBLICATION_YEAR = report_and_publication_date()\n",
        "\n",
        "REVERSE_REPLACEMENTS = {\n",
        "    ' января ': '.01.',\n",
        "    ' февраля ': '.02.',\n",
        "    ' марта ': '.03.',\n",
        "    ' апреля ': '.04.',\n",
        "    ' мая ': '.05.',\n",
        "    ' июня ': '.06.',\n",
        "    ' июля ': '.07.',\n",
        "    ' августа ': '.08.',\n",
        "    ' сентября ': '.09.',\n",
        "    ' октября ': '.10.',\n",
        "    ' ноября ': '.11.',\n",
        "    ' декабря ': '.12.'}\n",
        "REPLACEMENTS = {\n",
        "    '\\xa0': '',\n",
        "    '\\t': '',\n",
        "    '&nbsp': '',\n",
        "    ' январь ': '.01.',\n",
        "    ' января ': '.01.',\n",
        "    ' февраль ': '.02.',\n",
        "    ' февраля ': '.02.',\n",
        "    ' марта ': '.03.',\n",
        "    ' март ': '.03.',\n",
        "    ' апреля ': '.04.',\n",
        "    ' апрель ': '.04.',\n",
        "    ' май ': '.05.',\n",
        "    ' мая ': '.05.',\n",
        "    ' июнь ': '.06.',\n",
        "    ' июня ': '.06.',\n",
        "    ' июль ': '.07.',\n",
        "    ' июля ': '.07.',\n",
        "    ' августа ': '.08.',\n",
        "    ' август ': '.08.',\n",
        "    ' сентябрь ': '.09.',\n",
        "    ' сентября ': '.09.',\n",
        "    ' октябрь ': '.10.',\n",
        "    ' октября ': '.10.',\n",
        "    ' ноябрь ': '.11.',\n",
        "    ' ноября ': '.11.',\n",
        "    ' декабрь ': '.12.',\n",
        "    ' декабря ': '.12.',\n",
        "    '.21': '.2021',\n",
        "    '.22': '.2022',\n",
        "    '-': '.',\n",
        "    '01.(': '01.{}'.format(REPORT_DATE[-4:]),\n",
        "    '02.(': '02.{}'.format(REPORT_DATE[-4:]),\n",
        "    '03.(': '03.{}'.format(REPORT_DATE[-4:]),\n",
        "    '04.(': '04.{}'.format(REPORT_DATE[-4:]),\n",
        "    '05.(': '05.{}'.format(REPORT_DATE[-4:]),\n",
        "    '06.(': '06.{}'.format(REPORT_DATE[-4:]),\n",
        "    '07.(': '07.{}'.format(REPORT_DATE[-4:]),\n",
        "    '08.(': '08.{}'.format(REPORT_DATE[-4:]),\n",
        "    '09.(': '09.{}'.format(REPORT_DATE[-4:]),\n",
        "    '10.(': '10.{}'.format(REPORT_DATE[-4:]),\n",
        "    '11.(': '11.{}'.format(REPORT_DATE[-4:]),\n",
        "    '12.(': '12.{}'.format(REPORT_DATE[-4:]),\n",
        "}\n",
        "MONTH_DICT = {\n",
        "    '.01.': '31.01.',\n",
        "    '.02.': '28.02.',\n",
        "    '.03.': '31.03.',\n",
        "    '.04.': '30.04.',\n",
        "    '.05.': '31.05.',\n",
        "    '.06.': '30.06.',\n",
        "    '.07.': '31.07.',\n",
        "    '.08.': '31.08.',\n",
        "    '.09.': '30.09.',\n",
        "    '.10.': '31.10.',\n",
        "    '.11.': '30.11.',\n",
        "    '.12.': '31.12.'\n",
        "}\n",
        "REVERSE_REPLACEMENTS_IM = {\n",
        "    ' январь ': '.01.',\n",
        "    ' февраль ': '.02.',\n",
        "    ' март ': '.03.',\n",
        "    ' апрель ': '.04.',\n",
        "    ' май ': '.05.',\n",
        "    ' июнь ': '.06.',\n",
        "    ' июль ': '.07.',\n",
        "    ' август ': '.08.',\n",
        "    ' сентябрь ': '.09.',\n",
        "    ' октябрь ': '.10.',\n",
        "    ' ноябрь ': '.11.',\n",
        "    ' декабрь ': '.12.',\n",
        "}\n",
        "\n",
        "REPORT_DATE_TEXT, REPORT_DATE_TEXT_WO_DAY, PUBLICATION_MONTH_TEXT = all_datetypes_for_searching()\n",
        "patterns = ['собственных средств', 'размер сс', 'расчет сс', 'размера сс', 'собственные средства',\n",
        "            'собственный средства']"
      ],
      "metadata": {
        "id": "lgoEW2ayydmw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main.py\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "#from process_soup import form_raw_list_rss\n",
        "#from correct_links import correct_site_link, full_download_link\n",
        "#from settings import *\n",
        "#from download_manager import start_wd, download_lxml, download_error_lxml\n",
        "#from data_post_processing import yes_no_result, correct_publication_dates\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    download_mode = 0\n",
        "#    df = pd.read_excel('inputs/list_managementcompanies.xlsx')\n",
        "    df = pd.read_excel('list_managementcompanies.xlsx')\n",
        "\n",
        "    small_correction = df.index[df['Ссылка на РСС'].str.contains(\"profit-garant.ru/\")].to_list()[0]  # dynamic link\n",
        "    df.loc[small_correction, 'Ссылка на РСС'] = df.loc[small_correction, 'Ссылка на РСС'] + REPORT_DATE[4]\n",
        "\n",
        "    exceptions = df['Exceptions'].to_list()\n",
        "    site_links = correct_site_link(df['Страница в сети Internet'].to_list())\n",
        "\n",
        "    if download_mode == 1:\n",
        "        download_lxml(df['Ссылка на РСС'], LXML_PATH, browser=start_wd())\n",
        "        download_error_lxml(LXML_PATH, browser=start_wd())\n",
        "    download_link_list = []  # lists for filling\n",
        "    publication_list = []\n",
        "    bool_publication_on_report_date = []\n",
        "    for i in range(len(df.index)):\n",
        "        print(f'lxml index {i}')\n",
        "        try:\n",
        "            try:\n",
        "                with open(LXML_PATH + str(i) + 'page.lxml', 'r', encoding='utf-8') as f:\n",
        "                    page = f.read()\n",
        "            except UnicodeDecodeError:\n",
        "                with open(LXML_PATH + str(i) + 'page.lxml', 'r', encoding='cp1251') as f:\n",
        "                    page = f.read()\n",
        "            soup = bs(page.lower(), 'lxml')\n",
        "            soup_up = bs(page, 'lxml')\n",
        "            print(f'EXCEPTION FOR LMXL {exceptions[i]}')\n",
        "            pub_str, href_str = form_raw_list_rss(soup, soup_up, exceptions[i])\n",
        "            link = full_download_link(href_str, i, site_links)\n",
        "        except FileNotFoundError:\n",
        "            pub_str = '-'\n",
        "            link = '-'\n",
        "        finally:\n",
        "            download_link_list.append(link)\n",
        "            publication_list.append(pub_str)\n",
        "    df = df.drop(['ОГРН', 'Номер лицензии', 'Дата предоставления (начала действия) лицензии', 'Срок действия лицензии',\n",
        "                  'Адрес юридического лица', 'Телефоны', 'Полное (фирменное) наименование' \\\n",
        "                    ' управляющей компании инвестиционных фондов, паевых инвестиционных фондов и негосударственных '\n",
        "                                                         'пенсионных фондов'], axis=1)\n",
        "    df[f'Раскрыто на {REPORT_DATE}'] = yes_no_result(download_link_list, bool_publication_on_report_date)\n",
        "    df['Дата раскрытия'] = correct_publication_dates(publication_list)\n",
        "    df['Ссылка на скачивание'] = download_link_list\n",
        "    #df.to_excel('results/result_floor7_' + REPORT_DATE + '_lxml.xlsx')\n",
        "    df.to_excel('result_floor7_' + REPORT_DATE + '_lxml.xlsx')"
      ],
      "metadata": {
        "id": "BtYSNrTVg2vm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}